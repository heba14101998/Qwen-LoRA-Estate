{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["x4v0D3WgVvB8","_l-cfEqOVzuF","zFlH_4lZ1i6j","EHk9VZ_lEzNG"],"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7981839,"sourceType":"datasetVersion","datasetId":3202774},{"sourceId":11718898,"sourceType":"datasetVersion","datasetId":7348439},{"sourceId":238628299,"sourceType":"kernelVersion"},{"sourceId":238764139,"sourceType":"kernelVersion"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b16163efaa3467aac8c6c3a381acc04":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33466bd6149a4381b88c4fd4f96d441f","placeholder":"​","style":"IPY_MODEL_88bd985ff0b241d29edf9d93f83fb6ce","value":"Kaggle credentials successfully validated."}},"0fc862d1f32d4222a9eab0d3d3363952":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3f73d6e7d64e568520935df91bc862":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"290db3966a504b47bf6e23814a0b59c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33466bd6149a4381b88c4fd4f96d441f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f924d63bc0243798fcc0dcf91f640ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c6f83c2e6f41c9aeb0ba8cfd0c1e3d","placeholder":"​","style":"IPY_MODEL_5ca49aef822a4aa8acd9a7d8efa73235","value":"Connecting..."}},"504a7821b59d4524b299cd127e34865c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535e2f372f74492dbdfccf95fa2ccc51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bca663d78c5b48c3a8a5cbab44403501","placeholder":"​","style":"IPY_MODEL_e56dc41c9e1a4922849bb6f3ac85385d","value":"\n<b>Thank You</b></center>"}},"5ca49aef822a4aa8acd9a7d8efa73235":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6edbbdb4f52b4a4d84c5ae9c986ce926":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"TextModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_b198176b8666463bb22fc74613ca81c7","placeholder":"​","style":"IPY_MODEL_85a6a112fef043869b3f3f26a9afde08","value":"hebamo7amed"}},"74c6f83c2e6f41c9aeb0ba8cfd0c1e3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bca60effe184c59a2f2f9722f402d49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85a6a112fef043869b3f3f26a9afde08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88bd985ff0b241d29edf9d93f83fb6ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"949aca8fdc584623b1e7d456d4f33bd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"9593f1f003a64cf9a96900362576b76a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"9ec64b1bfc0647bb96bf803c2068d7b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_0fc862d1f32d4222a9eab0d3d3363952","style":"IPY_MODEL_9593f1f003a64cf9a96900362576b76a","tooltip":""}},"b198176b8666463bb22fc74613ca81c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bca663d78c5b48c3a8a5cbab44403501":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca360e1b101d4395af434fc52908477b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_0b16163efaa3467aac8c6c3a381acc04"],"layout":"IPY_MODEL_949aca8fdc584623b1e7d456d4f33bd9"}},"da45d9a4491f4bb49a94d3a6f390e904":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_504a7821b59d4524b299cd127e34865c","placeholder":"​","style":"IPY_MODEL_1c3f73d6e7d64e568520935df91bc862","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"e56dc41c9e1a4922849bb6f3ac85385d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff752a123aec447cbc4ee24fd02fa347":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_290db3966a504b47bf6e23814a0b59c7","placeholder":"​","style":"IPY_MODEL_7bca60effe184c59a2f2f9722f402d49","value":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Base models Qwen3-06B and Gemini API\nThis notebook is a **baseline evaluation** to assess the performance of pre-trained models (`Qwen3-065B` and `gemini-2.0-flash-exp` API) on the real estate price prediction task. \n\nThe notebook performs the following steps:\n\n1. Load dataset from remote (Kaggle/Hugging Face).\n2. Preprocess dataset.\n3. Evaluate pre-trained `Qwen\\Qwen3-0.6B` model using regression metrics.\n4. Evaluate Gemini API using regression metrics.\n5. Save results to compare with the **fine-tuned LoRa `Qwen3-0.6B` model**.","metadata":{}},{"cell_type":"markdown","source":"---\n## Setup\n---","metadata":{"id":"ZU5_1OLopMUy"}},{"cell_type":"markdown","source":"### **Install Dependencies**","metadata":{"id":"x4v0D3WgVvB8"}},{"cell_type":"code","source":"# !pip install -qU transformers wandb google-generativeai huggingface_hub[hf_xet]\n!pip install -qU  json_repair","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj6qTXvPpwyk","outputId":"9bd2220a-0e37-4a3f-b594-7b1bdf031cae","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:55:47.290895Z","iopub.execute_input":"2025-05-09T13:55:47.291528Z","iopub.status.idle":"2025-05-09T13:55:52.427258Z","shell.execute_reply.started":"2025-05-09T13:55:47.291500Z","shell.execute_reply":"2025-05-09T13:55:52.425873Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### **Import Dependencies**","metadata":{"id":"_l-cfEqOVzuF"}},{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport json_repair\nimport pandas as pd\nfrom IPython.display import Markdown, JSON\nfrom tqdm import tqdm\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport google.generativeai as genai\n\nfrom utils import logging_config, evalute_model, timeit\nfrom fine_tuning_helpers import apply_prompt_template, decode_response","metadata":{"id":"mAre3Yb9TEj9","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:55:52.429311Z","iopub.execute_input":"2025-05-09T13:55:52.429602Z","iopub.status.idle":"2025-05-09T13:55:59.503467Z","shell.execute_reply.started":"2025-05-09T13:55:52.429574Z","shell.execute_reply":"2025-05-09T13:55:59.502481Z"},"scrolled":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"os.environ['LOGS'] = '/kaggle/working/logs'\nos.environ['RESULTS'] = '/kaggle/working/results'\n\nos.makedirs(os.environ['LOGS'], exist_ok=True )\nos.makedirs(os.environ['RESULTS'], exist_ok=True )\n\nLOGGER = logging_config(log_dir=os.environ['LOGS'])\nLOGGER","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:10:30.934084Z","iopub.execute_input":"2025-05-09T14:10:30.934665Z","iopub.status.idle":"2025-05-09T14:10:30.944627Z","shell.execute_reply.started":"2025-05-09T14:10:30.934631Z","shell.execute_reply":"2025-05-09T14:10:30.943961Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"<_Logger utils (INFO)>"},"metadata":{}}],"execution_count":67},{"cell_type":"markdown","source":"### **Define Tokens and Authenticate**","metadata":{}},{"cell_type":"code","source":"# If using kaggle \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\ngemini_token = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n# uncomment if using colab\n# from google.colab import userdata\n# hf_token = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n# gemini_token = userdata.get(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:59:41.222999Z","iopub.execute_input":"2025-05-09T13:59:41.223299Z","iopub.status.idle":"2025-05-09T13:59:41.563036Z","shell.execute_reply.started":"2025-05-09T13:59:41.223277Z","shell.execute_reply":"2025-05-09T13:59:41.562297Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from huggingface_hub import whoami, login\n\n# !huggingface-cli login --token {mytoken} # another method\nlogin(token = hf_token)\ngenai.configure(api_key=gemini_token)\nJSON(whoami())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:59:43.141352Z","iopub.execute_input":"2025-05-09T13:59:43.141677Z","iopub.status.idle":"2025-05-09T13:59:43.377510Z","shell.execute_reply.started":"2025-05-09T13:59:43.141653Z","shell.execute_reply":"2025-05-09T13:59:43.376615Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.JSON object>","application/json":{"type":"user","id":"64ce9cd07c24890fb4a7417d","name":"heba1998","fullname":"Heba ","canPay":false,"periodEnd":null,"isPro":false,"avatarUrl":"/avatars/8145446f31c46dc9df026753db8f5d9c.svg","orgs":[],"auth":{"type":"access_token","accessToken":{"displayName":"fine-tune","role":"fineGrained","createdAt":"2025-05-06T20:44:04.749Z","fineGrained":{"canReadGatedRepos":true,"global":["discussion.write","post.write"],"scoped":[{"entity":{"_id":"681ada3852431b1769c74a39","type":"dataset","name":"heba1998/real-estate-data-for-llm-fine-tuning"},"permissions":["repo.content.read","discussion.write","repo.write"]},{"entity":{"_id":"64ce9cd07c24890fb4a7417d","type":"user","name":"heba1998"},"permissions":["user.webhooks.read","repo.content.read","repo.write","inference.serverless.write","inference.endpoints.infer.write","inference.endpoints.write","user.webhooks.write","collection.read","collection.write","discussion.write","user.billing.read"]}]}}}}},"metadata":{"application/json":{"expanded":false,"root":"root"}}}],"execution_count":40},{"cell_type":"code","source":"# uncomment if using colab\n# import kagglehub\n# kagglehub.login(validate_credentials=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["ca360e1b101d4395af434fc52908477b","da45d9a4491f4bb49a94d3a6f390e904","6edbbdb4f52b4a4d84c5ae9c986ce926","ff752a123aec447cbc4ee24fd02fa347","9ec64b1bfc0647bb96bf803c2068d7b5","535e2f372f74492dbdfccf95fa2ccc51","949aca8fdc584623b1e7d456d4f33bd9","504a7821b59d4524b299cd127e34865c","1c3f73d6e7d64e568520935df91bc862","b198176b8666463bb22fc74613ca81c7","85a6a112fef043869b3f3f26a9afde08","290db3966a504b47bf6e23814a0b59c7","7bca60effe184c59a2f2f9722f402d49","0fc862d1f32d4222a9eab0d3d3363952","9593f1f003a64cf9a96900362576b76a","bca663d78c5b48c3a8a5cbab44403501","e56dc41c9e1a4922849bb6f3ac85385d","4f924d63bc0243798fcc0dcf91f640ae","74c6f83c2e6f41c9aeb0ba8cfd0c1e3d","5ca49aef822a4aa8acd9a7d8efa73235","0b16163efaa3467aac8c6c3a381acc04","33466bd6149a4381b88c4fd4f96d441f","88bd985ff0b241d29edf9d93f83fb6ce"]},"id":"-PA52fS9nN3f","outputId":"53053baf-1b08-4315-b578-df27ae312e89","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Load Dataset\n---","metadata":{"id":"o-B1psKPpKQ9"}},{"cell_type":"markdown","source":"### Read data from remote (Kaggle/Hugging Face)","metadata":{}},{"cell_type":"markdown","source":"**Download the dataset from Kaggle**","metadata":{"id":"5Ix7pknmVppL"}},{"cell_type":"code","source":"# # Uncomment if using colab\n\n# kagglehub.dataset_download('hebamo7amed/real-estate-data-for-llm-fine-tuning')\n# tabular_data_path = f\"{data_path}/tabular_data\"\n# text_data_path = f\"{data_path}/text_data\"\n# text_data_path","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"0L9nwgUziDkG","outputId":"8654634a-1dd1-4b11-9f73-ba03b9f14ec0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Read Text Datasets**","metadata":{"id":"qPq5gXZvZ-5x"}},{"cell_type":"code","source":"# with open(f\"{text_data_path}/text_train_data.jsonl\", \"r\") as f:\n#   train_data = json.load(f)\n\n# with open(f\"{text_data_path}/text_val_data.jsonl\", \"r\") as f:\n#   val_data = json.load(f)\n\n# with open(f\"{text_data_path}/sample_50.jsonl\", \"r\") as f:\n#   sample_data = json.load(f)\n\n# print(\"Training data size = \", len(train_data))\n# print(\"Validation data size = \", len(val_data))\n# print(\"Sample data size = \", len(sample_data))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e38k8TCHUkf6","outputId":"38acf928-6ee3-465b-ff27-2d48fc413361","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Load Dataset Sample from hugging Face Hub**\n\nA data sample that was created from structured real estate data and uploaded to Hugging Face in the first notebook. It is formatted for instruction-based fine-tuning an LLM.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\n    path  ='heba1998/real-estate-data-sample-for-llm-fine-tuning'\n)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:00:51.051397Z","iopub.execute_input":"2025-05-09T14:00:51.051791Z","iopub.status.idle":"2025-05-09T14:00:55.950636Z","shell.execute_reply.started":"2025-05-09T14:00:51.051762Z","shell.execute_reply":"2025-05-09T14:00:55.949764Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d269fd21c87483cbb030c04e925d097"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"llm_train_data.jsonl:   0%|          | 0.00/5.54M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81891e740c944af497cf49e6b159ee9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"llm_val_data.jsonl:   0%|          | 0.00/222k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e4dabb539a4424983f10e4a2973b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb3aff9db244940979973ff4c679295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"670931b081454e57a6075d9fdbf77e62"}},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['system', 'instruction', 'input', 'output', 'history'],\n        num_rows: 5000\n    })\n    validation: Dataset({\n        features: ['system', 'instruction', 'input', 'output', 'history'],\n        num_rows: 200\n    })\n})"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# Convert data to list of jsons or ``jsonl``\n\nval_data = [sample for sample in dataset['validation']]\nhouse_price = lambda sample : json_repair.loads(sample['output'])['estimated_house_price']\ntrue_labels = [ house_price(sample) for sample in dataset['validation']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:12:47.182008Z","iopub.execute_input":"2025-05-09T14:12:47.182968Z","iopub.status.idle":"2025-05-09T14:12:47.213014Z","shell.execute_reply.started":"2025-05-09T14:12:47.182934Z","shell.execute_reply":"2025-05-09T14:12:47.211853Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"---\n## Evaluate Responses from Base LM `Qwen3-0.6B-Instruct`\n---","metadata":{"id":"2XEQiv5qe6yp"}},{"cell_type":"markdown","source":"### **Helper Function to get responses for base model**\n\nThis function uses another function from the `fine_tuning_helpers.py` and `utils` utility scripts.","metadata":{}},{"cell_type":"code","source":"@timeit\ndef batch_generate(model, tokenizer, data, device):\n    predictions = []\n    tokens_history = []\n    bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n    \n    for idx, sample in enumerate(tqdm(data, total=len(data),\n                                 desc=\"Get response from pre-trained `Qwen3-0.6B` model\",\n                                 ncols=100, colour='green')):\n        # 1. PREPROCESSING: \n        # build the system and user prompt with the model chat template\n        prompt = apply_prompt_template(sample, output_str='instruction')\n\n        # 2. TOKENIZATION: Tokenize the text prompt message\n        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n        n_input_tokens = len(inputs.input_ids[0])\n        LOGGER.info(f\"\\t>> Tokenized to {n_input_tokens} tokens\")\n        \n        # 3. GENERATION: Generate response\n        response_tokens_ids = model.generate(\n            inputs=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n        )\n        n_output_tokens = len(response_tokens_ids[0])\n\n        # 4. POSTPROCESSING: Return only the output tokens and  exclude input ids and then clean response\n        response_text = decode_response(response_tokens_ids, inputs.input_ids, tokenizer)\n        response_dict = json_repair.loads(response_text)            \n        house_price = response_dict[\"estimated_house_price\"]\n        predictions.append(int(house_price))\n        \n        # Store BAse model metadata for cost calculation\n        tokens_history.append({\n                'id': idx,\n                'input_tokens': n_input_tokens, \n                'output_tokens': n_output_tokens,\n                'total_tokens': n_input_tokens + n_output_tokens\n            })\n            \n    return predictions, tokens_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:14:34.792980Z","iopub.execute_input":"2025-05-09T14:14:34.793346Z","iopub.status.idle":"2025-05-09T14:14:34.801718Z","shell.execute_reply.started":"2025-05-09T14:14:34.793318Z","shell.execute_reply":"2025-05-09T14:14:34.800784Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"### **Load `Qwen3-0.6B` from Hugging Face**","metadata":{}},{"cell_type":"code","source":"model_id = \"Qwen/Qwen3-0.6B\"\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the model and tokenizer\ntokenizer_qwen = AutoTokenizer.from_pretrained(model_id)\nmodel_qwen = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=None).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:41:22.176417Z","iopub.execute_input":"2025-05-09T11:41:22.176780Z","iopub.status.idle":"2025-05-09T11:41:56.544316Z","shell.execute_reply.started":"2025-05-09T11:41:22.176752Z","shell.execute_reply":"2025-05-09T11:41:56.542716Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f4205dd0a0e484381864816bafbf857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d729e8b8560b45788b6f89add15c7a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd01da4120ac4a4cb8e4a6ae202e8499"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd47dd6d86d4f81a447b190ad72b9f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ea6da21fb7a48f5aeac19b8cc44e119"}},"metadata":{}},{"name":"stderr","text":"2025-05-09 11:41:33.445210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746790893.732709      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746790893.808796      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0728ffd0da584087a848ddbe03bd425d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02aee0dab7a04d6db0ad3a206ddbf6ff"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### **Generate Response using Pre-trained `Qwen3-0.6B`.**\n\nGenerate responses from the base model `Qwen3-0.6B` in the validation dataset.","metadata":{}},{"cell_type":"code","source":"# Make batch prediction or text to number genration \nqwen_preds, qwen_tokens_history = batch_generate(model=model_qwen, \n                                                  tokenizer=tokenizer_qwen, \n                                                  data = val_data,\n                                                  device = device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:41:56.545952Z","iopub.execute_input":"2025-05-09T11:41:56.546687Z","iopub.status.idle":"2025-05-09T12:01:54.393988Z","shell.execute_reply.started":"2025-05-09T11:41:56.546655Z","shell.execute_reply":"2025-05-09T12:01:54.392517Z"}},"outputs":[{"name":"stderr","text":"Get response from pre-trained `Qwen3-0.6B` model: 100%|\u001b[32m███████████\u001b[0m| 200/200 [19:57<00:00,  5.99s/it]\u001b[0m","output_type":"stream"},{"name":"stdout","text":"\n Data completed in 19.96 minutes.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"> Prediction for 200 sample takes 19.96 minutes using pretrained `Qwen3-0.6B`.","metadata":{}},{"cell_type":"markdown","source":"### **Evaluation metrics**\n\nThis function is implemented in the `utils.py` utility script.","metadata":{"id":"K3OCT8N8JWBX"}},{"cell_type":"code","source":"print(\"Actual Price\", true_labels[:10])\nprint(\"Predicted Prices\", qwen_preds[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:40:19.129366Z","iopub.execute_input":"2025-05-09T13:40:19.129871Z","iopub.status.idle":"2025-05-09T13:40:19.154883Z","shell.execute_reply.started":"2025-05-09T13:40:19.129838Z","shell.execute_reply":"2025-05-09T13:40:19.153690Z"}},"outputs":[{"name":"stdout","text":"Actual Price [2500000.0, 295000.0, 299900.0, 699000.0, 239000.0, 11000.0, 470000.0, 449000.0, 250000.0, 339000.0]\nPredicted Prices [55000, 128000, 85000, 85000, 85000, 85000, 85000, 85000, 85000, 85000]\n","output_type":"stream"}],"execution_count":211},{"cell_type":"code","source":"pretrained_qwen_metrics = evalute_model(true_labels, qwen_preds)\nJSON(pretrained_qwen_metrics)","metadata":{"id":"v0DpDsevIw4p","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:50:38.872037Z","iopub.execute_input":"2025-05-09T13:50:38.872351Z","iopub.status.idle":"2025-05-09T13:50:38.879434Z","shell.execute_reply.started":"2025-05-09T13:50:38.872328Z","shell.execute_reply":"2025-05-09T13:50:38.878626Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.JSON object>","application/json":{"MSE":428368963146.25,"RMSE":654499.0169177109,"MAE":404009.03,"R2":-0.5385257769487695}},"metadata":{"application/json":{"expanded":false,"root":"root"}}}],"execution_count":7},{"cell_type":"markdown","source":"> Model produce bad result and don't follow the output schema\n\n> Model predict the same value each time `85000` or `8500` because it is bais towards the example of the schema.","metadata":{"id":"rKJ7qhbwir3n"}},{"cell_type":"code","source":"qwen_results = pd.DataFrame(qwen_tokens_history)\nqwen_results['y_actual'] = true_labels\nqwen_results['y_pred'] = qwen_preds\n\nqwen_results ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:57:33.960565Z","iopub.execute_input":"2025-05-09T13:57:33.961209Z","iopub.status.idle":"2025-05-09T13:57:33.993443Z","shell.execute_reply.started":"2025-05-09T13:57:33.961180Z","shell.execute_reply":"2025-05-09T13:57:33.992657Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"      id  input_tokens  output_tokens  total_tokens   y_actual  y_pred\n0      0           244            264           508  2500000.0   55000\n1      1           246            266           512   295000.0  128000\n2      2           245            265           510   299900.0   85000\n3      3           246            266           512   699000.0   85000\n4      4           245            265           510   239000.0   85000\n..   ...           ...            ...           ...        ...     ...\n195  195           230            250           480   349000.0   85000\n196  196           241            261           502   220000.0  210000\n197  197           244            264           508   389000.0   85000\n198  198           231            251           482   582990.0   85000\n199  199           229            249           478   220000.0   85000\n\n[200 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>input_tokens</th>\n      <th>output_tokens</th>\n      <th>total_tokens</th>\n      <th>y_actual</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>244</td>\n      <td>264</td>\n      <td>508</td>\n      <td>2500000.0</td>\n      <td>55000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>246</td>\n      <td>266</td>\n      <td>512</td>\n      <td>295000.0</td>\n      <td>128000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>245</td>\n      <td>265</td>\n      <td>510</td>\n      <td>299900.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>246</td>\n      <td>266</td>\n      <td>512</td>\n      <td>699000.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>245</td>\n      <td>265</td>\n      <td>510</td>\n      <td>239000.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>195</td>\n      <td>230</td>\n      <td>250</td>\n      <td>480</td>\n      <td>349000.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>196</td>\n      <td>241</td>\n      <td>261</td>\n      <td>502</td>\n      <td>220000.0</td>\n      <td>210000</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>197</td>\n      <td>244</td>\n      <td>264</td>\n      <td>508</td>\n      <td>389000.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>198</td>\n      <td>231</td>\n      <td>251</td>\n      <td>482</td>\n      <td>582990.0</td>\n      <td>85000</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>199</td>\n      <td>229</td>\n      <td>249</td>\n      <td>478</td>\n      <td>220000.0</td>\n      <td>85000</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"### **Save Pre-trained`Qwen3-0.6B` Results**","metadata":{}},{"cell_type":"code","source":"with open(f\"{os.environ['RESULTS']}/pretrained_qwen_metrics.json\", 'w') as json_file:\n    json.dump(pretrained_qwen_metrics, json_file, indent=4)\n\nqwen_results.to_csv(f\"{os.environ['RESULTS']}/pretrained_qwen_results.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T13:57:36.395418Z","iopub.execute_input":"2025-05-09T13:57:36.395729Z","iopub.status.idle":"2025-05-09T13:57:36.408689Z","shell.execute_reply.started":"2025-05-09T13:57:36.395684Z","shell.execute_reply":"2025-05-09T13:57:36.407735Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"---\n## Evaluate Responses of Gemini Model\n---\n\nEvaluate the responses from Gemini API using regression metrics.","metadata":{"id":"KQQ4gDLjLJ8V"}},{"cell_type":"markdown","source":"### **Helper Function to get responses from `genai` SDK API**\n\nThis function uses another function from the `fine_tuning_helpers.py` and `utils` utility scripts.","metadata":{}},{"cell_type":"code","source":"from fine_tuning_helpers import extract_house_price\n\n@timeit\ndef batch_api_generate(model, data):\n    llm_predictions = []\n    tokens_history = []\n    bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n    \n    for idx, sample in enumerate(tqdm(data, total=len(data),\n                                 desc=\"Get response from pre-trained `Qwen3-0.6B` model\",\n                                 ncols=100, colour='green')):\n        LOGGER.info(\"-\"*50)\n        LOGGER.info(f\"Sample id {idx}\")\n        LOGGER.info(\"-\"*50)\n        # 1. PREPROCESSING: \n        # build the system and user prompt with the model chat template\n        prompt= apply_prompt_template(sample)\n\n        # 2. GENERATION: Generate response\n        response = model.generate_content(prompt)\n        text_response = response.candidates[0].content.parts[0].text\n\n        # 4. POSTPROCESSING:  return only the output tokens and  exclude input ids\n        LOGGER.info(f\"Response text for sample id={idx} \\n {text_response}\" )\n        \n        # Clean response\n        house_price = extract_house_price(text_response)\n        LOGGER.info(f\"\\t>> Predicted price: {house_price}\")\n        llm_predictions.append(int(house_price))\n        \n        # Store metadata for expenses calculation\n        tokens_history.append({\n                'id': idx,\n                'input_tokens': response.usage_metadata.prompt_token_count, \n                'output_tokens': response.usage_metadata.candidates_token_count,\n                'total_tokens': response.usage_metadata.total_token_count\n            })\n            \n        LOGGER.debug(f\"Updated tokens history for {idx}\")\n\n    return llm_predictions, tokens_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:01:51.393402Z","iopub.execute_input":"2025-05-09T14:01:51.393745Z","iopub.status.idle":"2025-05-09T14:01:51.401369Z","shell.execute_reply.started":"2025-05-09T14:01:51.393698Z","shell.execute_reply":"2025-05-09T14:01:51.400489Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"list(model.name for model in genai.list_models())","metadata":{"id":"iBkMuyuKB7Jg","trusted":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-05-09T12:23:06.908242Z","iopub.execute_input":"2025-05-09T12:23:06.908612Z","iopub.status.idle":"2025-05-09T12:23:07.186377Z","shell.execute_reply.started":"2025-05-09T12:23:06.908588Z","shell.execute_reply":"2025-05-09T12:23:07.185489Z"},"collapsed":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['models/chat-bison-001',\n 'models/text-bison-001',\n 'models/embedding-gecko-001',\n 'models/gemini-1.0-pro-vision-latest',\n 'models/gemini-pro-vision',\n 'models/gemini-1.5-pro-latest',\n 'models/gemini-1.5-pro-001',\n 'models/gemini-1.5-pro-002',\n 'models/gemini-1.5-pro',\n 'models/gemini-1.5-flash-latest',\n 'models/gemini-1.5-flash-001',\n 'models/gemini-1.5-flash-001-tuning',\n 'models/gemini-1.5-flash',\n 'models/gemini-1.5-flash-002',\n 'models/gemini-1.5-flash-8b',\n 'models/gemini-1.5-flash-8b-001',\n 'models/gemini-1.5-flash-8b-latest',\n 'models/gemini-1.5-flash-8b-exp-0827',\n 'models/gemini-1.5-flash-8b-exp-0924',\n 'models/gemini-2.5-pro-exp-03-25',\n 'models/gemini-2.5-pro-preview-03-25',\n 'models/gemini-2.5-flash-preview-04-17',\n 'models/gemini-2.5-flash-preview-04-17-thinking',\n 'models/gemini-2.5-pro-preview-05-06',\n 'models/gemini-2.0-flash-exp',\n 'models/gemini-2.0-flash',\n 'models/gemini-2.0-flash-001',\n 'models/gemini-2.0-flash-lite-001',\n 'models/gemini-2.0-flash-lite',\n 'models/gemini-2.0-flash-lite-preview-02-05',\n 'models/gemini-2.0-flash-lite-preview',\n 'models/gemini-2.0-pro-exp',\n 'models/gemini-2.0-pro-exp-02-05',\n 'models/gemini-exp-1206',\n 'models/gemini-2.0-flash-thinking-exp-01-21',\n 'models/gemini-2.0-flash-thinking-exp',\n 'models/gemini-2.0-flash-thinking-exp-1219',\n 'models/learnlm-1.5-pro-experimental',\n 'models/learnlm-2.0-flash-experimental',\n 'models/gemma-3-1b-it',\n 'models/gemma-3-4b-it',\n 'models/gemma-3-12b-it',\n 'models/gemma-3-27b-it',\n 'models/embedding-001',\n 'models/text-embedding-004',\n 'models/gemini-embedding-exp-03-07',\n 'models/gemini-embedding-exp',\n 'models/aqa',\n 'models/imagen-3.0-generate-002',\n 'models/gemini-2.0-flash-live-001']"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"### **Generate Response using Gemini.**\n\nGenerate responses from `gemini-2.0-flash-exp` in the validation dataset.","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nimport numpy as np\n\nConfigs = genai.GenerationConfig(max_output_tokens=200)\n\nmodel_id = \"models/gemini-2.0-flash-exp\"\ngemini = genai.GenerativeModel(model_name=model_id,\n                               generation_config=Configs)\n\ngemini_preds, gemini_tokens_history = batch_api_generate(gemini, data = val_data)","metadata":{"id":"oyp-D90YCdIN","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:01:55.600246Z","iopub.execute_input":"2025-05-09T14:01:55.600585Z","iopub.status.idle":"2025-05-09T14:04:22.839202Z","shell.execute_reply.started":"2025-05-09T14:01:55.600511Z","shell.execute_reply":"2025-05-09T14:04:22.838157Z"}},"outputs":[{"name":"stderr","text":"Get response from pre-trained `Qwen3-0.6B` model: 100%|\u001b[32m███████████\u001b[0m| 200/200 [02:27<00:00,  1.36it/s]\u001b[0m","output_type":"stream"},{"name":"stdout","text":"\n Data completed in 2.45 minutes.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"### **Evaluation metrics**\n\nThis function is implemented in the `utils.py` utility script.","metadata":{"id":"K3OCT8N8JWBX"}},{"cell_type":"code","source":"print(\"Actual Price\", true_labels[:10])\nprint(\"Predicted Prices\", gemini_preds[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:04:22.840787Z","iopub.execute_input":"2025-05-09T14:04:22.841117Z","iopub.status.idle":"2025-05-09T14:04:22.846651Z","shell.execute_reply.started":"2025-05-09T14:04:22.841094Z","shell.execute_reply":"2025-05-09T14:04:22.845687Z"}},"outputs":[{"name":"stdout","text":"Actual Price [2500000.0, 295000.0, 299900.0, 699000.0, 239000.0, 11000.0, 470000.0, 449000.0, 250000.0, 339000.0]\nPredicted Prices [-1, -1, 345000, -1, -1, -1, -1, -1, -1, -1]\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"> **`-1`** indecate to that gemini didn't produce the result.","metadata":{}},{"cell_type":"code","source":"gemini_metrics = evalute_model(true_labels, gemini_preds)\n\nJSON(gemini_metrics)","metadata":{"id":"v0DpDsevIw4p","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:04:22.847755Z","iopub.execute_input":"2025-05-09T14:04:22.848139Z","iopub.status.idle":"2025-05-09T14:04:22.873172Z","shell.execute_reply.started":"2025-05-09T14:04:22.848118Z","shell.execute_reply":"2025-05-09T14:04:22.872322Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.JSON object>","application/json":{"MSE":404732979411.82,"RMSE":636186.2772897731,"MAE":390199.94,"R2":-0.4536350090185375}},"metadata":{"application/json":{"expanded":false,"root":"root"}}}],"execution_count":54},{"cell_type":"code","source":"gemini_results = pd.DataFrame(gemini_tokens_history)\ngemini_results['y_actual'] = true_labels\ngemini_results['y_pred'] = gemini_preds\n\nmissed_prec = len(gemini_results[gemini_results['y_pred']==-1]) *100 /len(val_data)\nprint(f\">>>>>> Gemini can't predict {missed_prec}% from the given data <<<<<<\")\n\ngemini_results.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:04:56.673126Z","iopub.execute_input":"2025-05-09T14:04:56.673411Z","iopub.status.idle":"2025-05-09T14:04:56.687456Z","shell.execute_reply.started":"2025-05-09T14:04:56.673392Z","shell.execute_reply":"2025-05-09T14:04:56.686522Z"}},"outputs":[{"name":"stdout","text":">>>>>> Gemini can't predict 72.0% from the given data <<<<<<\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"   id  input_tokens  output_tokens  total_tokens   y_actual  y_pred\n0   0           257             78           335  2500000.0      -1\n1   1           258             77           335   295000.0      -1\n2   2           256            104           360   299900.0  345000\n3   3           258             78           336   699000.0      -1\n4   4           258             78           336   239000.0      -1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>input_tokens</th>\n      <th>output_tokens</th>\n      <th>total_tokens</th>\n      <th>y_actual</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>257</td>\n      <td>78</td>\n      <td>335</td>\n      <td>2500000.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>258</td>\n      <td>77</td>\n      <td>335</td>\n      <td>295000.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>256</td>\n      <td>104</td>\n      <td>360</td>\n      <td>299900.0</td>\n      <td>345000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>258</td>\n      <td>78</td>\n      <td>336</td>\n      <td>699000.0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>258</td>\n      <td>78</td>\n      <td>336</td>\n      <td>239000.0</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"### **Save Gemini Results**","metadata":{}},{"cell_type":"code","source":"with open(f\"{os.environ['RESULTS']}/gemini_metrics.json\", 'w') as json_file:\n    json.dump(gemini_metrics, json_file, indent=4)\n\nqwen_results.to_csv(f\"{os.environ['RESULTS']}/gemini_results.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:22:42.607877Z","iopub.execute_input":"2025-05-09T14:22:42.608285Z","iopub.status.idle":"2025-05-09T14:22:42.616302Z","shell.execute_reply.started":"2025-05-09T14:22:42.608259Z","shell.execute_reply":"2025-05-09T14:22:42.615312Z"}},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":"### **Save Results**","metadata":{}},{"cell_type":"code","source":"import json\n\nresults_metadata = {\n    \"Description\": \"Here is the time taken to predict 200 samples for the base models\",\n    \"Qwen Base (min)\": 2.45,\n    \"Gemini (min)\": 19.96\n}\n\n\nwith open(f\"{os.environ['RESULTS']}/results_metadata.json\", 'w') as json_file:\n    json.dump(results_metadata, json_file, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:22:44.061985Z","iopub.execute_input":"2025-05-09T14:22:44.062274Z","iopub.status.idle":"2025-05-09T14:22:44.068154Z","shell.execute_reply.started":"2025-05-09T14:22:44.062255Z","shell.execute_reply":"2025-05-09T14:22:44.066980Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# to be downolad for further comparasions\n!zip /kaggle/working/results_200.zip /kaggle/working/results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T14:22:46.914104Z","iopub.execute_input":"2025-05-09T14:22:46.914416Z","iopub.status.idle":"2025-05-09T14:22:47.061963Z","shell.execute_reply.started":"2025-05-09T14:22:46.914393Z","shell.execute_reply":"2025-05-09T14:22:47.060791Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/results/ (stored 0%)\n","output_type":"stream"}],"execution_count":73}]}